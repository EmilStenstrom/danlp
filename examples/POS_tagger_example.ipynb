{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Danish Part of Speech Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show you how to get started using Polyglots opens source POS taggger and a POS tagger that is trained with the framework from Flair (Zalando). The Flair POS-tagger is trained by this project and is included in the DaNLP packgage.  \n",
    "\n",
    "#### Credits:  \n",
    "Polyglot: https://polyglot.readthedocs.io/en/latest/POS.html  \n",
    "Flair: https://github.com/zalandoresearch/flair  \n",
    "Data from UD_Danish: https://github.com/UniversalDependencies/UD_Danish-DDT/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get started\n",
    "\n",
    "#### Installation\n",
    "To run all the example in this notebook the following python packages are required, which can be installed using pip:\n",
    "\n",
    "```pip install danlp``` Read more about the package on the front page of this Github repositorie. \n",
    "\n",
    "```pip install polyglot==16.7.4``` \n",
    "Read more about Polyglot installation [here](https://polyglot.readthedocs.io/en/latest/Installation.html). Notice that polyglot requires you to install other dependencies, such as pyicu, pycld2 and Morfessor. Note that the polyglot package is of older date.\n",
    "\n",
    "```pip install flair==0.4.2 ```\n",
    "Read more about Flair installation [here](https://pypi.org/project/flair/)\n",
    "\n",
    "It is recommended to install the packages in an virtual envoriment usign e.g. pip virtual envoriment. Read more about it [here](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First load and import\n",
    "1) Import polyglot  and download the Polyglot part of speech tagging model through polyglot\n",
    "\n",
    "2) Import flair and download the Flair part of speech tagging model trained in DaNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package embeddings2.da to\n",
      "[polyglot_data]     /root/polyglot_data...\n",
      "[polyglot_data] Downloading package pos2.da to /root/polyglot_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POLYGLOT import libraries and load model\n",
    "from polyglot.text import Text\n",
    "# download the Danish part of speech tagger from Polyglot\n",
    "from polyglot.downloader import downloader\n",
    "downloader.download(\"embeddings2.da\")\n",
    "downloader.download(\"pos2.da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 09:20:00,967 loading file /root/.danlp/flair.pos.pt\n"
     ]
    }
   ],
   "source": [
    "# FLAIR import libraries and load model\n",
    "from danlp.models.pos_taggers import load_pos_tagger_with_flair\n",
    "from flair.data import Sentence, Token\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from segtok.segmenter import split_single\n",
    "\n",
    "# Load the POS tagger using the DaNLP wrapper\n",
    "flair_model = load_pos_tagger_with_flair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simply.. \n",
    "This shows the simple use of the two frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30mThe flair Model:\u001b[0m\n",
      "jeg <PRON> hopper <VERB> på <ADP> en <DET> bil <NOUN> som <ADP> er <AUX> rød <ADJ> sammen <ADV> med <ADP> Jens-Peter <PROPN> E. <PROPN> Hansen <PROPN>\n",
      "\n",
      "\u001b[1;30mThe Polyglot model: \u001b[0m\n",
      "[('jeg', 'PRON'), ('hopper', 'VERB'), ('på', 'ADP'), ('en', 'PRON'), ('bil', 'NOUN'), ('som', 'PART'), ('er', 'VERB'), ('rød', 'ADJ'), ('sammen', 'ADV'), ('med', 'ADP'), ('Jens', 'PROPN'), ('-', 'PUNCT'), ('Peter', 'PROPN'), ('E', 'PROPN'), ('.', 'PUNCT'), ('Hansen', 'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "# Giv a sentence to try it on\n",
    "example='jeg hopper på en bil som er rød sammen med Jens-Peter E. Hansen'\n",
    "\n",
    "# The Flair model\n",
    "print('\\x1b[1;30m'+'The flair Model:' +'\\x1b[0m')\n",
    "sentence = Sentence(example) \n",
    "flair_model.predict(sentence) \n",
    "print(sentence.to_tagged_string())\n",
    "print()\n",
    "\n",
    "# The Polyglot model\n",
    "print('\\x1b[1;30m'+'The Polyglot model: ' +'\\x1b[0m')\n",
    "text = Text(example, hint_language_code='da')\n",
    "print(text.pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example above you might have notice that the two models are not alligned. The polyglot is for example splitting the name 'Jens-Peter' into three tokens where the flair model is not. Futhermore, it looks like the flair Model have som tags like 'AUX', as the Polyglot model does not use.  \n",
    "\n",
    "The Flair model is trained on the Danish Dependency Treebank ( UD_Danish) dataset, the Polyglot documentations say this as well, but a suggestion is that the polyglot might have been trained on a older version with a slighly different tokenization, and perhaps 14 tags instead og 17 tags. \n",
    "\n",
    "You can load the test (or training) data in the following way, and eximine the ground truth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'UD_Danish-DDT'...\n",
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 185 (delta 1), reused 4 (delta 1), pack-reused 180\u001b[K\n",
      "Receiving objects: 100% (185/185), 4.07 MiB | 6.12 MiB/s, done.\n",
      "Resolving deltas: 100% (106/106), done.\n"
     ]
    }
   ],
   "source": [
    "# Download the the Danish UD treebank  (if you wnat to run nest cell)\n",
    "!git clone  https://github.com/UniversalDependencies/UD_Danish-DDT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaliepauli/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated function (or staticmethod) read_conll_ud. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To spanske TV-folk blev gennembanket af Sovjet-soldater , fordi de havde filmet en såret litauer , der blev hjulpet væk fra slagmarken .\n",
      "\n",
      "\u001b[1;30mThe ground truth from the annotated data\u001b[0m\n",
      "To <NUM> spanske <ADJ> TV-folk <NOUN> blev <AUX> gennembanket <VERB> af <ADP> Sovjet-soldater <NOUN> , <PUNCT> fordi <SCONJ> de <PRON> havde <AUX> filmet <VERB> en <DET> såret <VERB> litauer <NOUN> , <PUNCT> der <PRON> blev <AUX> hjulpet <VERB> væk <ADV> fra <ADP> slagmarken <NOUN> . <PUNCT>\n",
      "\n",
      "\u001b[1;30mPolyglots predictions:\u001b[0m\n",
      "[('To', 'NUM'), ('spanske', 'ADJ'), ('TV', 'NOUN'), ('-', 'PUNCT'), ('folk', 'NOUN'), ('blev', 'VERB'), ('gennembanket', 'ADJ'), ('af', 'ADP'), ('Sovjet', 'PROPN'), ('-', 'PUNCT'), ('soldater', 'NOUN'), (',', 'PUNCT'), ('fordi', 'SCONJ'), ('de', 'PRON'), ('havde', 'VERB'), ('filmet', 'VERB'), ('en', 'PRON'), ('såret', 'ADJ'), ('litauer', 'ADV'), (',', 'PUNCT'), ('der', 'PART'), ('blev', 'VERB'), ('hjulpet', 'VERB'), ('væk', 'ADV'), ('fra', 'ADP'), ('slagmarken', 'NOUN'), ('.', 'PUNCT')]\n",
      "\n",
      "\u001b[1;30mFlairs predictions:\u001b[0m\n",
      "To <NUM> spanske <ADJ> TV-folk <NOUN> blev <AUX> gennembanket <VERB> af <ADP> Sovjet-soldater <NOUN> , <PUNCT> fordi <SCONJ> de <PRON> havde <AUX> filmet <VERB> en <DET> såret <VERB> litauer <NOUN> , <PUNCT> der <PRON> blev <AUX> hjulpet <VERB> væk <ADV> fra <ADP> slagmarken <NOUN> . <PUNCT>\n"
     ]
    }
   ],
   "source": [
    "# read the test dataset from the Danish UD treebank \n",
    "sentences=NLPTaskDataFetcher.read_conll_ud('UD_Danish-DDT/da_ddt-ud-test.conllu') \n",
    "\n",
    "# Eksemple\n",
    "eksemple=sentences[97].to_plain_string()\n",
    "print(eksemple)\n",
    "\n",
    "# the ground truth \n",
    "print()\n",
    "print('\\x1b[1;30m'+'The ground truth from the annotated data'+'\\x1b[0m')\n",
    "print(sentences[97].to_tagged_string('upos'))\n",
    "\n",
    "# Polyglot\n",
    "print()\n",
    "print('\\x1b[1;30m'+'Polyglots predictions:'+'\\x1b[0m')\n",
    "poly_sentence = Text(eksemple, hint_language_code='da')\n",
    "print(poly_sentence.pos_tags)\n",
    "\n",
    "#Flair\n",
    "print()\n",
    "print('\\x1b[1;30m'+'Flairs predictions:'+'\\x1b[0m')\n",
    "flair_sentence = Sentence(eksemple)\n",
    "flair_model.predict(flair_sentence)\n",
    "print(flair_sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets play a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictornary\n",
    "\n",
    "color_dict = {\n",
    "    'ADJ': \"\\033[1;30;43m\",\n",
    "    'ADP': \"\\033[1;30;45m\",\n",
    "    'ADV': \"\\033[1;30;105m\",\n",
    "    'AUX': \"\\033[1;30;101m\",\n",
    "    'CONJ': \"\\033[1;30;103m\",\n",
    "    'DET': \"\\033[1;30;46m\",\n",
    "    'INTJ': \"\\033[1;37;46m\",\n",
    "    'NOUN': \"\\033[1;30;42m\",\n",
    "    'NUM': \"\\033[1;37;100m\",\n",
    "    'PART':  \"\\033[1;30;47m\",\n",
    "    'PRON': \"\\033[1;30;104m\",\n",
    "    'PROPN': \"\\033[1;30;44m\",\n",
    "    'PUNCT':\"\\033[1;30;102m\",\n",
    "    'SCONJ': \"\\033[1;37;103m\",\n",
    "    'SYM': \"\\033[1;37;102m\",\n",
    "    'VERB': \"\\033[1;30;41m\",\n",
    "    'X': \"\\033[1;37;47m\"\n",
    "}\n",
    "\n",
    "explanation_dict = {\n",
    "    'ADJ': 'Adjective',\n",
    "    'ADP': 'Adposition',\n",
    "    'ADV': 'Adverb',\n",
    "    'AUX': 'Auxiliary verb',\n",
    "    'CONJ': 'Coordinating conjunction',\n",
    "    'DET': 'Determiner',\n",
    "    'INTJ': 'Interjection',\n",
    "    'NOUN': 'Noun',\n",
    "    'NUM': 'Numeral',\n",
    "    'PART':  'Particle',\n",
    "    'PRON': 'Pronoun',\n",
    "    'PROPN': 'Proper noun',\n",
    "    'PUNCT':'Punctuation',\n",
    "    'SCONJ': 'Subordinating conjunction',\n",
    "    'SYM': 'Symbol',\n",
    "    'VERB': 'Verb',\n",
    "    'X': 'Other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_the_tags(sentence, color_dict, explanation_dict,flair_model):\n",
    "    # flair prediction\n",
    "    flair_sentence = Sentence(sentence)\n",
    "    flair_model.predict(flair_sentence)\n",
    "    \n",
    "    # Showing in color\n",
    "    print('\\033[1;30;m\\n The tagged sentence:\\n')\n",
    "    pos_color_list = [color_dict.get(str(token.tags['upos']).split(' ')[0]) + token.text for token in  flair_sentence]\n",
    "    print(' '.join(pos_color_list)) \n",
    "\n",
    "    # list of the used tags\n",
    "    tag_list = [str(token.tags['upos']).split(' ')[0] for token in flair_sentence]\n",
    "\n",
    "    # get the explanation in colors\n",
    "    exp_color_list = [color_dict.get(tag) + explanation_dict.get(tag) for tag in  tag_list]\n",
    "\n",
    "    # remove duplicate explanation but keep the order\n",
    "    exp_color_list = sorted(set(exp_color_list), key=lambda x: exp_color_list.index(x))\n",
    "    print('\\033[1;30;m\\n Explanations:\\n')\n",
    "    # print the explanation\n",
    "    print('\\033[1;30;m '.join(exp_color_list))  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;m\n",
      " The tagged sentence:\n",
      "\n",
      "\u001b[1;30;41mvis \u001b[1;30;104mmig \u001b[1;30;43malle \u001b[1;30;42mordklasserne\n",
      "\u001b[1;30;m\n",
      " Explanations:\n",
      "\n",
      "\u001b[1;30;41mVerb\u001b[1;30;m \u001b[1;30;104mPronoun\u001b[1;30;m \u001b[1;30;43mAdjective\u001b[1;30;m \u001b[1;30;42mNoun\n"
     ]
    }
   ],
   "source": [
    "show_the_tags('vis mig alle ordklasserne', color_dict, explanation_dict, flair_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface variabels\n",
    "\n",
    "Count the pos tags in a text. This can be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 2,\n",
       " 'ADV': 5,\n",
       " 'AUX': 2,\n",
       " 'CONJ': 0,\n",
       " 'DET': 2,\n",
       " 'INTJ': 0,\n",
       " 'NOUN': 3,\n",
       " 'NUM': 0,\n",
       " 'PART': 0,\n",
       " 'PRON': 4,\n",
       " 'PROPN': 0,\n",
       " 'PUNCT': 3,\n",
       " 'SCONJ': 0,\n",
       " 'SYM': 0,\n",
       " 'VERB': 2,\n",
       " 'X': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look also a the tutorials on Flairs github page:\n",
    "# https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md\n",
    "\n",
    "# many sentences\n",
    "text = \"Dette her er en sætning. Detter her er en anden. Find hvormange af hver ordklasserne der er i denne tekst.\"\n",
    "# or load form for example from file\n",
    "\n",
    "# split the text into sentences for the flair framwork\n",
    "sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\n",
    "\n",
    "# predict the pos tags for the list of sentences\n",
    "flair_model.predict(sentences) # for large text set the size of mini batches \n",
    "all_tags=[str(token.tags['upos']).split(' ')[0] for sent in sentences for token in sent]\n",
    "\n",
    "# initialize a dictonary to for counting\n",
    "count_dict = {\n",
    "    'ADJ': 0,\n",
    "    'ADP': 0,\n",
    "    'ADV': 0,\n",
    "    'AUX': 0,\n",
    "    'CONJ': 0,\n",
    "    'DET': 0,\n",
    "    'INTJ': 0,\n",
    "    'NOUN': 0,\n",
    "    'NUM': 0,\n",
    "    'PART':  0,\n",
    "    'PRON': 0,\n",
    "    'PROPN': 0,\n",
    "    'PUNCT':0,\n",
    "    'SCONJ': 0,\n",
    "    'SYM': 0,\n",
    "    'VERB': 0,\n",
    "    'X': 0\n",
    "}\n",
    "# count the tags and save in the dictonary\n",
    "count_dict={k:all_tags.count(k) for (k,v) in count_dict.items()}\n",
    "count_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
